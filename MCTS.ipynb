{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import queue\n",
    "import math\n",
    "from enum import Enum\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'D:\\\\Projects\\\\MCTS\\\\datasets\\\\'\n",
    "file_names = [#{'name':'blood-transfusion-service-center.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'credit-g.csv', 'target_class': 'class', 'pos_class': 'good'},\n",
    "             {'name':'kr-vs-kp.csv', 'target_class': 'class', 'pos_class': '\\'won\\''},\n",
    "             {'name':'monks-problems-2.csv', 'target_class': 'class', 'pos_class': 'numerical'},\n",
    "             {'name':'diabetes.csv', 'target_class': 'class', 'pos_class': 'tested_positive'},\n",
    "             {'name':'qsar-biodeg.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'steel-plates-fault.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'tic-tac-toe.csv', 'target_class': 'Class', 'pos_class': 'positive'},\n",
    "             {'name':'wdbc.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'hill-valley.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'pc1.csv', 'target_class': 'defects', 'pos_class': 'true'},\n",
    "             {'name':'spambase.csv', 'target_class': 'class', 'pos_class': 'numerical'},\n",
    "             {'name':'artificial.csv', 'target_class': 'class', 'pos_class': 'numerical'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method for preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_info, pos_label = None):\n",
    "    data = pd.read_csv(data_path + file_info['name'])  \n",
    "    \n",
    "    if pos_label is None:\n",
    "        pos_label = max(data.loc[:,file_info['target_class']])\n",
    "\n",
    "    data.loc[data.loc[:,file_info['target_class']] != pos_label, file_info['target_class']] = 0\n",
    "    data.loc[data.loc[:,file_info['target_class']] == pos_label, file_info['target_class']] = 1\n",
    "    \n",
    "    labels = data.loc[:,file_info['target_class']]\n",
    "    data = data.drop(columns = [file_info['target_class']])\n",
    "    \n",
    "    return (data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildInMetrics:\n",
    "    def __init__(self):\n",
    "        self._metrics = {\n",
    "            'acc': accuracy_score,\n",
    "            'f1': f1_score,\n",
    "            'roc_auc': roc_auc_score\n",
    "        }\n",
    "        \n",
    "    def get_metric(self, name):\n",
    "        if name in self._metrics:\n",
    "            return self._metrics[name]\n",
    "        else:\n",
    "            print('Error initializing MCTS object, \\\"' + name + '\\\" is not supported metric, available values are: ' + ', '.join(self._metrics.keys()))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringFunctions():\n",
    "    def __init__(self, name, params):\n",
    "        self._params = params\n",
    "        self._name = name\n",
    "    \n",
    "    def set_scoring_function(self, name, params = None):\n",
    "        self._name = name\n",
    "        if(params is not None):\n",
    "            self._params = params \n",
    "    \n",
    "    def get_score(self, node, scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self._default_scoring(node)\n",
    "        elif(self._name == 'with_variance'):\n",
    "            return self._var_scoring(node)\n",
    "        elif(self._name == 'g_rave'):\n",
    "            return self._g_rave_scoring(node, scores)\n",
    "        else:\n",
    "            print('Error initializing MCTS object, \\\"' + + '\\\"')\n",
    "            return None\n",
    "    \n",
    "    def _default_scoring(self, node):\n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            return float(\"Inf\")\n",
    "        else:\n",
    "            return node.get_score() + math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T)\n",
    "        \n",
    "    def _var_scoring(self, node):\n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            return float(\"Inf\")\n",
    "        else:\n",
    "            return node.get_score() + math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T)\n",
    "    \n",
    "    def _g_rave_scoring(self, node, scores):\n",
    "        g_scores = scores['g_rave']\n",
    "        \n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            if(node.feature_name not in g_scores):\n",
    "                return float(\"Inf\")\n",
    "            else:\n",
    "                return g_scores[node.feature_name]['score']\n",
    "        else:\n",
    "            c = self._params['c']\n",
    "            c_l = self._params['c_l']\n",
    "            alpha = c/(c + node.T)\n",
    "            beta = c_l/(c_l + node.T)\n",
    "            \n",
    "            return ((1 - alpha) * node.get_score() + \n",
    "                alpha * g_scores[node.feature_name]['score'] + \n",
    "                math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T) *\n",
    "                min(0.25, node.get_variance() + math.sqrt(2 * math.log(node._parent_node.T)/node.T)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalScores:\n",
    "    def __init__(self):\n",
    "        self.scores = {'g_rave': {}}\n",
    "    \n",
    "    def update_g_rave_score(self, name, score):\n",
    "        if(name not in self.scores['g_rave']):\n",
    "            self.scores['g_rave'][name] = {'n': 1, 'score': score}\n",
    "        else:\n",
    "            n = self.scores['g_rave'][name]['n']\n",
    "            t_score = (self.scores['g_rave'][name]['score'] * n + score)/(n + 1)\n",
    "            self.scores['g_rave'][name] = {'n': n + 1, 'score': t_score}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_name, parent_node = None, is_subtree_full = False):\n",
    "        self.feature_name = feature_name\n",
    "        self.child_nodes = []\n",
    "        self.T = 0\n",
    "        self.score_sum = 0\n",
    "        self._is_subtree_full = is_subtree_full\n",
    "        self._parent_node = parent_node\n",
    "        self._scores = []\n",
    "        \n",
    "    def add_child_node(self, node_name, is_subtree_full = False):\n",
    "        new_node = Node(node_name, self, is_subtree_full)\n",
    "        self.child_nodes.append(new_node)\n",
    "        \n",
    "    def add_child_nodes(self, node_names):\n",
    "        for name in node_names:\n",
    "            self.add_child_node(name, len(node_names) == 1)\n",
    "      \n",
    "    def update_node(self, score, scores):\n",
    "        self.score_sum += score\n",
    "        self.T += 1\n",
    "        self._scores.append(score)\n",
    "        scores.update_g_rave_score(self.feature_name, score)\n",
    "        if(len(self.child_nodes) != 0):\n",
    "            is_subtree_full = True\n",
    "            for node in self.child_nodes:\n",
    "                if(not node._is_subtree_full):\n",
    "                    is_subtree_full = False\n",
    "                    break\n",
    "            self._is_subtree_full = is_subtree_full\n",
    "    \n",
    "    def get_score(self):\n",
    "        return float('Inf') if self.T == 0 else self.score_sum/self.T\n",
    "    \n",
    "    def get_variance(self):\n",
    "        return np.var(self._scores)\n",
    "    \n",
    "    def update_scores_up(self, score, scores):\n",
    "        current_node = self\n",
    "        \n",
    "        while(current_node != None):\n",
    "            current_node.update_node(score, scores)\n",
    "            current_node = current_node._parent_node  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiarm strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmStrategies:\n",
    "    def __init__(self, name, all_node_names):\n",
    "        self._name = name\n",
    "        self._all_node_names = all_node_names\n",
    "        \n",
    "    def multiarm_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self._default_strategy(node, used_features, scoring_function, other_scores)\n",
    "        else:\n",
    "            return None    \n",
    "    \n",
    "    def _default_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        #print(\"default strategy: \" + node.feature_name)\n",
    "        if(len(node.child_nodes) == 0):\n",
    "            #print(\"first if\")\n",
    "            self._add_child_nodes(node, used_features)\n",
    "            return node.child_nodes[0]\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            best_score = 0\n",
    "            best_node = None\n",
    "            tmp_score = 0\n",
    "            \n",
    "            for child_node in node.child_nodes:\n",
    "                if(not child_node._is_subtree_full):\n",
    "                    score = scoring_function(child_node, other_scores)\n",
    "                    if(score > best_score):\n",
    "                        best_score = score\n",
    "                        best_node = child_node\n",
    "            return best_node\n",
    "  \n",
    "    def _add_child_nodes(self, node, used_features):\n",
    "        #print('adding nodes to ' + node.feature_name + ' :' + ' '.join(self._all_node_names - used_features))\n",
    "        node.add_child_nodes(self._all_node_names - used_features)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EndStrategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndStrategies:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "    \n",
    "    def are_calculations_over(self, node, params):\n",
    "        if(self._name == 'default'):\n",
    "            return self._first_new_strategy(node, params)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _first_new_strategy(self, node, params):\n",
    "        #print(\"_first_new_strategy:\")\n",
    "        if(node.T > 0 and not node._is_subtree_full):\n",
    "            #print(\"first if\")\n",
    "            return False\n",
    "        else:\n",
    "            if(node._parent_node == None):\n",
    "                #print(\"second if, no parent node\")\n",
    "                return False\n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefaultSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultSettings:\n",
    "    @staticmethod\n",
    "    def get_default_params():\n",
    "        return {\n",
    "            \"c_e\": 2,\n",
    "            \"c\": 1,\n",
    "            \"c_l\": 1\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 task = 'classification',\n",
    "                 calculactions_done_conditions = {'type': 'iterations', 'max_val': 10},\n",
    "                 params = None,\n",
    "                 metric = 'acc', \n",
    "                 scoring_function = 'g_rave', \n",
    "                 multiarm_strategy = 'default', \n",
    "                 end_strategy = 'default'):\n",
    "        self._metric_name = metric\n",
    "        self._scoring_function_name = scoring_function\n",
    "        self._multiarm_strategy_name = multiarm_strategy\n",
    "        self._end_strategy_name = end_strategy\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._task = task\n",
    "        self._root = Node(\"\")\n",
    "        self._feature_names = None\n",
    "        self._calculactions_done_conditions = calculactions_done_conditions\n",
    "        self._model = model\n",
    "        self._time = 0\n",
    "        self._iterations = 0\n",
    "        \n",
    "        if(params is None):\n",
    "            self._params = DefaultSettings.get_default_params()\n",
    "        \n",
    "    def fit(self, data, out_variable):\n",
    "        data = data.reset_index(drop=True)\n",
    "        out_variable = out_variable.reset_index(drop=True)\n",
    "        if(self._task == 'classification'):\n",
    "            self._classification_fit(data, out_variable)\n",
    "        else:\n",
    "            self._regression_fit(data, out_variable)\n",
    "            \n",
    "    def _classification_fit(self, data, out_variable):\n",
    "        self._init_fitting_values(data)\n",
    "        while not self._is_fitting_over():\n",
    "            self._single_classification_iteration(data, out_variable)\n",
    "        \n",
    "        self._model.fit(data.loc[:, self._best_features], out_variable)\n",
    "    \n",
    "    def _regression_fit(self, data, out_variable):\n",
    "        return None\n",
    "    \n",
    "    def _single_classification_iteration(self, data, out_variable):\n",
    "        #print('classification iteration')\n",
    "        used_features = set()\n",
    "        node = self._root\n",
    "        is_iteration_over = False\n",
    "        while not is_iteration_over:\n",
    "            node = self._multiarm_strategy.multiarm_strategy(node, used_features, self._scoring_function.get_score, self._global_scores.scores)\n",
    "            #print(\"selected feature: \" + node.feature_name)\n",
    "            is_iteration_over = self._end_strategy.are_calculations_over(node, self._params)\n",
    "            used_features.add(node.feature_name)\n",
    "            #print(\"--------\")\n",
    "        \n",
    "        score = self._cv_score(data.loc[:,used_features], out_variable)\n",
    "        #print('score: ' + str(score))\n",
    "        node.update_scores_up(score, self._global_scores)\n",
    "        #print(used_features)\n",
    "        if(score > self._best_score):\n",
    "            self._best_score = score\n",
    "            self._best_features = used_features\n",
    "        \n",
    "        if(self._longest_tree_branch < len(used_features)):\n",
    "            self._longest_tree_branch = len(used_features)\n",
    "        \n",
    "        #print(\"----------END OF ITERATION----------\")\n",
    "    \n",
    "    def _single_regression_iteration(self):\n",
    "        return None\n",
    "    \n",
    "    def _init_fitting_values(self, data):\n",
    "        self._feature_names = set(data.columns)\n",
    "        self._multiarm_strategy = MultiArmStrategies(self._multiarm_strategy_name, self._feature_names)\n",
    "        self._end_strategy = EndStrategies(self._end_strategy_name)\n",
    "        self._scoring_function = ScoringFunctions(self._scoring_function_name, self._params)\n",
    "        self._metric = BuildInMetrics().get_metric(self._metric_name)\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._iterations = 0\n",
    "        self._longest_tree_branch = 0\n",
    "        self._time = time.time()\n",
    "        self._global_scores = GlobalScores()\n",
    "    \n",
    "    def _is_fitting_over(self):\n",
    "        if(self._root._is_subtree_full):\n",
    "            print('Whole tree searched, finishing prematurely')\n",
    "            return True\n",
    "        \n",
    "        if(self._calculactions_done_conditions['type'] == 'iterations'):\n",
    "            self._iterations += 1\n",
    "            return self._iterations > self._calculactions_done_conditions['max_val'] \n",
    "        else:\n",
    "            #print('Time ellapsed: ' + str(time.time() - self._time))\n",
    "            return (time.time() - self._time) > self._calculactions_done_conditions['max_val'] \n",
    "        \n",
    "    def _cv_score(self, data, labels):\n",
    "        kf = KFold(n_splits = 5, random_state = 123, shuffle=True)\n",
    "        score = 0\n",
    "        for train, test in kf.split(data):\n",
    "            self._model.fit(data.loc[train,:], labels[train])\n",
    "            predicted = self._model.predict(data.loc[test,:])\n",
    "            score += self._metric(labels[test], predicted)\n",
    "        return score/5\n",
    "    \n",
    "    def predict(self, data):\n",
    "        return self._model.predict(data.loc[:, self._best_features])\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        return self._model.predict_proba(data.loc[:, self._best_features])\n",
    "    \n",
    "    def get_features_importances(self):\n",
    "        return self._global_scores.scores['g_rave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + file_names[7]['name'])\n",
    "labels = data.loc[:,file_names[7]['target_class']]\n",
    "data = data.drop(columns = [file_names[7]['target_class']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-4f43f66fba11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-7b746dc8c63b>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(file_info, pos_label)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mpos_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtransformed_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfile_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'target_class'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not Series"
     ]
    }
   ],
   "source": [
    "a = get_data(file_names[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.010293381766029912\n",
      "acc: 0.9524607980127309\n",
      "f1: 0.9626405561889433\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, random_state = 123, shuffle=True)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "auc_score = 0\n",
    "acc_score = 0\n",
    "f1_score_ = 0\n",
    "for train, test in kf.split(data):\n",
    "    model.fit(data.loc[train,:], labels[train])\n",
    "    predicted = model.predict(data.loc[test,:])\n",
    "    p_proba = model.predict_proba(data.loc[test,:])[:,1]\n",
    "    auc_score += roc_auc_score(labels[test] == 1, p_proba)\n",
    "    acc_score += accuracy_score(labels[test], predicted)\n",
    "    f1_score_ += f1_score(labels[test], predicted, pos_label = 1)\n",
    "\n",
    "print('auc: ' + str(auc_score/5))\n",
    "print('acc: ' + str(acc_score/5))\n",
    "print('f1: ' + str(f1_score_/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.25, random_state = 123)\n",
    "mcts = MCTS(model, calculactions_done_conditions = {'type': 'time', 'max_val': 3600}, metric='acc')\n",
    "mcts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6513511782786885\n",
      "0.618\n",
      "0.6156941649899397\n"
     ]
    }
   ],
   "source": [
    "predicted = mcts.predict(X_test)\n",
    "p_proba = mcts.predict_proba(X_test)\n",
    "print(roc_auc_score(y_test == 1, p_proba[:,1]))\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print(f1_score(y_test, predicted, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x476'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts._best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

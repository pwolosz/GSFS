{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import queue\n",
    "import math\n",
    "from enum import Enum\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'D:\\\\Projects\\\\MCTS\\\\datasets\\\\'\n",
    "file_names = [{'name':'blood-transfusion-service-center.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'credit-g.csv', 'target_class': 'class', 'pos_class': 'good'},\n",
    "             {'name':'kr-vs-kp.csv', 'target_class': 'class', 'pos_class': '\\'won\\''},\n",
    "             {'name':'monks-problems-2.csv', 'target_class': 'class', 'pos_class': 'numerical'},\n",
    "             {'name':'diabetes.csv', 'target_class': 'class', 'pos_class': 'tested_positive'},\n",
    "             {'name':'qsar-biodeg.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'steel-plates-fault.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'tic-tac-toe.csv', 'target_class': 'Class', 'pos_class': 'positive'},\n",
    "             {'name':'wdbc.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'hill-valley.csv', 'target_class': 'Class', 'pos_class': 'numerical'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildInMetrics:\n",
    "    def __init__(self):\n",
    "        self.__metrics = {\n",
    "            'acc': accuracy_score\n",
    "        }\n",
    "        \n",
    "    def get_metric(self, name):\n",
    "        if name in self.__metrics:\n",
    "            return self.__metrics[name]\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringFunctions():\n",
    "    def __init__(self, name, params):\n",
    "        self._params = params\n",
    "        self._name = name\n",
    "    \n",
    "    def get_score(self, node, other_scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self.default_scoring(node)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def default_scoring(self, node):\n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            return float(\"Inf\")\n",
    "        else:\n",
    "            return node.get_score() + math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_name, parent_node = None, is_subtree_full = False):\n",
    "        self.feature_name = feature_name\n",
    "        self.child_nodes = []\n",
    "        self.T = 0\n",
    "        self.score_sum = 0\n",
    "        self._is_subtree_full = is_subtree_full\n",
    "        self._parent_node = parent_node\n",
    "        \n",
    "    def add_child_node(self, node_name, is_subtree_full = False):\n",
    "        new_node = Node(node_name, self, is_subtree_full)\n",
    "        self.child_nodes.append(new_node)\n",
    "        \n",
    "    def add_child_nodes(self, node_names):\n",
    "        for name in node_names:\n",
    "            self.add_child_node(name, len(node_names) == 1)\n",
    "      \n",
    "    def update_node(self, score):\n",
    "        self.score_sum += score\n",
    "        self.T += 1\n",
    "        \n",
    "        if(len(self.child_nodes) != 0):\n",
    "            is_subtree_full = True\n",
    "            for node in self.child_nodes:\n",
    "                if(not node._is_subtree_full):\n",
    "                    is_subtree_full = False\n",
    "                    break\n",
    "            self._is_subtree_full = is_subtree_full\n",
    "    \n",
    "    def get_score(self):\n",
    "        return float('Inf') if self.T == 0 else self.score_sum/self.T\n",
    "    \n",
    "    def update_scores_up(self, score):\n",
    "        current_node = self\n",
    "        \n",
    "        while(current_node != None):\n",
    "            current_node.update_node(score)\n",
    "            current_node = current_node._parent_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: Creating a Node\n",
    "print(\"Scenario 1:\")\n",
    "root = Node(\"\")\n",
    "print(root.feature_name == \"\")\n",
    "print(root.T == 0)\n",
    "print(root._is_subtree_full == False)\n",
    "print(root._parent_node == None)\n",
    "print(root.T == 0)\n",
    "print(root.get_score() == float('Inf'))\n",
    "\n",
    "# Scenario 2: Adding child nodes\n",
    "print(\"Scenario 2:\")\n",
    "root.add_child_nodes(['b','c','d'])\n",
    "print(len(root.child_nodes) == 3)\n",
    "print(root.child_nodes[0].feature_name == 'b')\n",
    "print(root.child_nodes[1].feature_name == 'c')\n",
    "print(root.child_nodes[2].feature_name == 'd')\n",
    "root.child_nodes[0].update_scores_up(1)\n",
    "print(root.child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].get_score() == 1)\n",
    "print(root.get_score() == 1)\n",
    "print(root.T == 1)\n",
    "root.child_nodes[1].update_scores_up(0.8)\n",
    "print(root.child_nodes[1].T == 1)\n",
    "print(root.child_nodes[1].get_score() == 0.8)\n",
    "print(root.get_score() == 0.9)\n",
    "print(root.T == 2)\n",
    "root.child_nodes[2].update_scores_up(0.6)\n",
    "print(root.child_nodes[2].T == 1)\n",
    "print(root.child_nodes[2].get_score() == 0.6)\n",
    "print(root.get_score() == 2.4/3)\n",
    "print(root.T == 3)\n",
    "\n",
    "# Another level\n",
    "print(\"Scenario 3\")\n",
    "root.child_nodes[0].add_child_nodes(['c','d'])\n",
    "print(len(root.child_nodes[0].child_nodes) == 2)\n",
    "print(root.child_nodes[0].child_nodes[0].feature_name == 'c')\n",
    "print(root.child_nodes[0].child_nodes[1].feature_name == 'd')\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full == False)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.2)\n",
    "print(root.child_nodes[0].child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].get_score() == 0.2)\n",
    "print(root.child_nodes[0].T == 2)\n",
    "print(root.child_nodes[0].get_score() == 0.6)\n",
    "print(root.T == 4)\n",
    "print(root.get_score() == 2.6/4)\n",
    "# Scenario 3: last \n",
    "print(\"Scenario 4:\")\n",
    "root.child_nodes[0].child_nodes[0].add_child_nodes(['d'])\n",
    "print(len(root.child_nodes[0].child_nodes[0].child_nodes) == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0]._is_subtree_full)\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full == False)\n",
    "root.child_nodes[0].child_nodes[0].child_nodes[0].update_scores_up(0.4)\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full)\n",
    "print(root.child_nodes[0].child_nodes[1]._is_subtree_full == False)\n",
    "print(root.child_nodes[0]._is_subtree_full == False)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0].get_score() == 0.4)\n",
    "print(root.child_nodes[0].child_nodes[0].T == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiarm strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmStrategies:\n",
    "    def __init__(self, name, all_node_names):\n",
    "        self._name = name\n",
    "        self._all_node_names = all_node_names\n",
    "        \n",
    "    def multiarm_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self._default_strategy(node, used_features, scoring_function, other_scores)\n",
    "        else:\n",
    "            return None    \n",
    "    \n",
    "    def _default_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        #print(\"default strategy: \" + node.feature_name)\n",
    "        if(len(node.child_nodes) == 0):\n",
    "            #print(\"first if\")\n",
    "            self._add_child_nodes(node, used_features)\n",
    "            return node.child_nodes[0]\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            best_score = 0\n",
    "            best_node = None\n",
    "            tmp_score = 0\n",
    "            \n",
    "            for child_node in node.child_nodes:\n",
    "                score = scoring_function(child_node, other_scores)\n",
    "                if(score > best_score):\n",
    "                    best_score = score\n",
    "                    best_node = child_node\n",
    "            return best_node\n",
    "  \n",
    "    def _add_child_nodes(self, node, used_features):\n",
    "        #print('adding nodes to ' + node.feature_name + ' :' + ' '.join(self._all_node_names - used_features))\n",
    "        node.add_child_nodes(self._all_node_names - used_features)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EndStrategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndStrategies:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "    \n",
    "    def are_calculations_over(self, node, params):\n",
    "        if(self._name == 'default'):\n",
    "            return self._first_new_strategy(node, params)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _first_new_strategy(self, node, params):\n",
    "        #print(\"_first_new_strategy:\")\n",
    "        if(node.T > 0 and not node._is_subtree_full):\n",
    "            #print(\"first if\")\n",
    "            return False\n",
    "        else:\n",
    "            if(node._parent_node == None):\n",
    "                #print(\"second if, no parent node\")\n",
    "                return False\n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefaultSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultSettings:\n",
    "    @staticmethod\n",
    "    def get_default_params():\n",
    "        return {\n",
    "            \"c_e\": 2\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, \n",
    "                 task,\n",
    "                 model,\n",
    "                 calculactions_done_conditions = {'type': 'iterations', 'max_val': 10},\n",
    "                 params = None,\n",
    "                 metric = 'acc', \n",
    "                 scoring_function = 'default', \n",
    "                 multiarm_strategy = 'default', \n",
    "                 end_strategy = 'default'):\n",
    "        self._metric_name = metric\n",
    "        self._scoring_function_name = scoring_function\n",
    "        self._multiarm_strategy_name = multiarm_strategy\n",
    "        self._end_strategy_name = end_strategy\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._task = task\n",
    "        self._root = Node(\"\")\n",
    "        self._feature_names = None\n",
    "        self._calculactions_done_conditions = calculactions_done_conditions\n",
    "        self._model = model\n",
    "        self._time = 0\n",
    "        self._iterations = 0\n",
    "        \n",
    "        if(params is None):\n",
    "            self._params = DefaultSettings.get_default_params()\n",
    "        \n",
    "    def fit(self, data, out_variable):\n",
    "        \n",
    "        if(self._task == 'classification'):\n",
    "            self._classification_fit(data, out_variable)\n",
    "        else:\n",
    "            self._regression_fit(data, out_variable)\n",
    "            \n",
    "    def _classification_fit(self, data, out_variable):\n",
    "        self._init_fitting_values()\n",
    "        while not self._is_fitting_over():\n",
    "            self._single_classification_iteration(data, out_variable)\n",
    "        \n",
    "        self._model.fit(data.loc[:, self._best_features], out_variable)\n",
    "    \n",
    "    def _regression_fit(self, data, out_variable):\n",
    "        return None\n",
    "    \n",
    "    def _single_classification_iteration(self, data, out_variable):\n",
    "        #print('classification iteration')\n",
    "        used_features = set()\n",
    "        node = self._root\n",
    "        is_iteration_over = False\n",
    "        while not is_iteration_over:\n",
    "            node = self._multiarm_strategy.multiarm_strategy(node, used_features, self._scoring_function.get_score, self._other_scores)\n",
    "            #print(\"selected feature: \" + node.feature_name)\n",
    "            is_iteration_over = self._end_strategy.are_calculations_over(node, self._params)\n",
    "            used_features.add(node.feature_name)\n",
    "            #print(\"--------\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.loc[:,used_features], out_variable, test_size = 0.25, random_state = 123)\n",
    "        self._model.fit(X_train, y_train)\n",
    "        predicted = self._model.predict(X_test)\n",
    "        score = self._metric(y_test, predicted)\n",
    "        #print('score: ' + str(score))\n",
    "        node.update_scores_up(score)\n",
    "        #print(used_features)\n",
    "        if(score > self._best_score):\n",
    "            self._best_score = score\n",
    "            self._best_features = used_features\n",
    "        \n",
    "        if(self._longest_tree_branch < len(used_features)):\n",
    "            self._longest_tree_branch = len(used_features)\n",
    "        \n",
    "        #print(\"----------END OF ITERATION----------\")\n",
    "    \n",
    "    def _single_regression_iteration(self):\n",
    "        return None\n",
    "    \n",
    "    def _init_fitting_values(self):\n",
    "        self._feature_names = set(data.columns)\n",
    "        self._multiarm_strategy = MultiArmStrategies(self._multiarm_strategy_name, self._feature_names)\n",
    "        self._end_strategy = EndStrategies(self._end_strategy_name)\n",
    "        self._scoring_function = ScoringFunctions(self._scoring_function_name, self._params)\n",
    "        self._metric = BuildInMetrics().get_metric(self._metric_name)\n",
    "        self._other_scores = {}\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._iterations = 0\n",
    "        self._longest_tree_branch = 0\n",
    "        self._time = time.time()\n",
    "    \n",
    "    def _is_fitting_over(self):\n",
    "        if(self._root._is_subtree_full):\n",
    "            print('Whole tree searched, finishing prematurely')\n",
    "            return True\n",
    "        \n",
    "        if(self._calculactions_done_conditions['type'] == 'iterations'):\n",
    "            self._iterations += 1\n",
    "            return self._iterations > self._calculactions_done_conditions['max_val'] \n",
    "        else:\n",
    "            #print('Time ellapsed: ' + str(time.time() - self._time))\n",
    "            return (time.time() - self._time) > self._calculactions_done_conditions['max_val'] \n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self._model.predict(data.loc[:, self._best_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + file_names[0]['name'])\n",
    "data_labels = data.loc[:,file_names[0]['target_class']]\n",
    "data = data.drop(columns = [file_names[0]['target_class']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_labels, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whole tree searched, finishing prematurely\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7593582887700535"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts = MCTS(\"classification\", XGBClassifier(), calculactions_done_conditions = {'type': 'time', 'max_val': 10})\n",
    "mcts.fit(X_train, y_train)\n",
    "predicted = mcts.predict(X_test)\n",
    "(predicted == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'V3', 'V4'}"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts._best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + file_names[9]['name'])\n",
    "data_labels = data.loc[:,file_names[9]['target_class']]\n",
    "data = data.drop(columns = [file_names[9]['target_class']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_labels, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcts = MCTS(\"classification\", XGBClassifier(), calculactions_done_conditions = {'type': 'time', 'max_val': 600})\n",
    "mcts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5313531353135313"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = mcts.predict(X_test)\n",
    "(predicted == y_test).sum()/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

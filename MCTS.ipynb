{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import queue\n",
    "import math\n",
    "from enum import Enum\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import random\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'D:\\\\Projects\\\\MCTS\\\\datasets\\\\'\n",
    "file_names = [{'name':'blood-transfusion-service-center.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'credit-g.csv', 'target_class': 'class', 'pos_class': 'good'},\n",
    "             {'name':'kr-vs-kp.csv', 'target_class': 'class', 'pos_class': '\\'won\\''},\n",
    "             {'name':'monks-problems-2.csv', 'target_class': 'class', 'pos_class': 'numerical'},\n",
    "             {'name':'diabetes.csv', 'target_class': 'class', 'pos_class': 'tested_positive'},\n",
    "             {'name':'qsar-biodeg.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'steel-plates-fault.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'tic-tac-toe.csv', 'target_class': 'Class', 'pos_class': 'positive'},\n",
    "             {'name':'wdbc.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'hill-valley.csv', 'target_class': 'Class', 'pos_class': 'numerical'},\n",
    "             {'name':'pc1.csv', 'target_class': 'defects', 'pos_class': 'true'},\n",
    "             {'name':'spambase.csv', 'target_class': 'class', 'pos_class': 'numerical'},\n",
    "             {'name':'artificial.csv', 'target_class': 'class', 'pos_class': 'numerical'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildInMetrics:\n",
    "    def __init__(self):\n",
    "        self._metrics = {\n",
    "            'acc': accuracy_score,\n",
    "            'f1': f1_score,\n",
    "            'roc_auc': roc_auc_score\n",
    "        }\n",
    "        \n",
    "    def get_metric(self, name):\n",
    "        if name in self._metrics:\n",
    "            return self._metrics[name]\n",
    "        else:\n",
    "            print('Error initializing MCTS object, \\\"' + name + '\\\" is not supported metric, available values are: ' + ', '.join(self._metrics.keys()))\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoringFunctions():\n",
    "    def __init__(self, name, params):\n",
    "        self._params = params\n",
    "        self._name = name\n",
    "    \n",
    "    def set_scoring_function(self, name, params = None):\n",
    "        self._name = name\n",
    "        if(params is not None):\n",
    "            self._params = params \n",
    "    \n",
    "    def get_score(self, node, scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self._default_scoring(node)\n",
    "        elif(self._name == 'with_variance'):\n",
    "            return self._var_scoring(node)\n",
    "        elif(self._name == 'g_rave'):\n",
    "            return self._g_rave_scoring(node, scores)\n",
    "        else:\n",
    "            print('Error initializing MCTS object, \\\"' + + '\\\"')\n",
    "            return None\n",
    "    \n",
    "    def _default_scoring(self, node):\n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            return float(\"Inf\")\n",
    "        else:\n",
    "            return node.get_score() + math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T)\n",
    "        \n",
    "    def _var_scoring(self, node):\n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            return float(\"Inf\")\n",
    "        else:\n",
    "            return node.get_score() + math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T)\n",
    "    \n",
    "    def _g_rave_scoring(self, node, scores):\n",
    "        g_scores = scores['g_rave']\n",
    "        \n",
    "        if(node._parent_node == None or node.T == 0):\n",
    "            if(node.feature_name not in g_scores):\n",
    "                return float(\"Inf\")\n",
    "            else:\n",
    "                return g_scores[node.feature_name]['score']\n",
    "        else:\n",
    "            c = self._params['c']\n",
    "            c_l = self._params['c_l']\n",
    "            alpha = c/(c + node.T)\n",
    "            beta = c_l/(c_l + node.T)\n",
    "            \n",
    "            return ((1 - alpha) * node.get_score() + \n",
    "                alpha * g_scores[node.feature_name]['score'] + \n",
    "                math.sqrt(self._params['c_e'] * math.log(node._parent_node.T)/node.T) *\n",
    "                min(0.25, node.get_variance() + math.sqrt(2 * math.log(node._parent_node.T)/node.T)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "sf = ScoringFunctions('g_rave', DefaultSettings.get_default_params())\n",
    "gs = GlobalScores()\n",
    "root = Node('')\n",
    "root.add_child_nodes(['a','b','c','d'])\n",
    "root.child_nodes[0].update_scores_up(1, gs)\n",
    "root.child_nodes[1].update_scores_up(0.8, gs)\n",
    "root.child_nodes[2].update_scores_up(0.6, gs)\n",
    "root.child_nodes[3].update_scores_up(0.4, gs)\n",
    "root.child_nodes[0].add_child_nodes(['b','c','d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.8,gs)\n",
    "root.child_nodes[1].update_scores_up(0.7,gs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.75\n",
      "0.7941176470588236\n",
      "1.7308183826022854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.1974104780035124"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sf.get_score(root.child_nodes[1], gs.scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalScores:\n",
    "    def __init__(self):\n",
    "        self.scores = {'g_rave': {}}\n",
    "    \n",
    "    def update_g_rave_score(self, name, score):\n",
    "        if(name not in self.scores['g_rave']):\n",
    "            self.scores['g_rave'][name] = {'n': 1, 'score': score}\n",
    "        else:\n",
    "            n = self.scores['g_rave'][name]['n']\n",
    "            t_score = (self.scores['g_rave'][name]['score'] * n + score)/(n + 1)\n",
    "            self.scores['g_rave'][name] = {'n': n + 1, 'score': t_score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GlobalScores()\n",
    "gs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Node class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, feature_name, parent_node = None, is_subtree_full = False):\n",
    "        self.feature_name = feature_name\n",
    "        self.child_nodes = []\n",
    "        self.T = 0\n",
    "        self.score_sum = 0\n",
    "        self._is_subtree_full = is_subtree_full\n",
    "        self._parent_node = parent_node\n",
    "        self._scores = []\n",
    "        \n",
    "    def add_child_node(self, node_name, is_subtree_full = False):\n",
    "        new_node = Node(node_name, self, is_subtree_full)\n",
    "        self.child_nodes.append(new_node)\n",
    "        \n",
    "    def add_child_nodes(self, node_names):\n",
    "        for name in node_names:\n",
    "            self.add_child_node(name, len(node_names) == 1)\n",
    "      \n",
    "    def update_node(self, score, scores):\n",
    "        self.score_sum += score\n",
    "        self.T += 1\n",
    "        self._scores.append(score)\n",
    "        scores.update_g_rave_score(self.feature_name, score)\n",
    "        if(len(self.child_nodes) != 0):\n",
    "            is_subtree_full = True\n",
    "            for node in self.child_nodes:\n",
    "                if(not node._is_subtree_full):\n",
    "                    is_subtree_full = False\n",
    "                    break\n",
    "            self._is_subtree_full = is_subtree_full\n",
    "    \n",
    "    def get_score(self):\n",
    "        return float('Inf') if self.T == 0 else self.score_sum/self.T\n",
    "    \n",
    "    def get_variance(self):\n",
    "        return np.var(self._scores)\n",
    "    \n",
    "    def update_scores_up(self, score, scores):\n",
    "        current_node = self\n",
    "        \n",
    "        while(current_node != None):\n",
    "            current_node.update_node(score, scores)\n",
    "            current_node = current_node._parent_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario 1:\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Scenario 2:\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Scenario 3\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "Scenario 4:\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Scenario 1: Creating a Node\n",
    "print(\"Scenario 1:\")\n",
    "root = Node(\"\")\n",
    "print(root.feature_name == \"\")\n",
    "print(root.T == 0)\n",
    "print(root._is_subtree_full == False)\n",
    "print(root._parent_node == None)\n",
    "print(root.T == 0)\n",
    "print(root.get_score() == float('Inf'))\n",
    "\n",
    "# Scenario 2: Adding child nodes\n",
    "print(\"Scenario 2:\")\n",
    "root.add_child_nodes(['b','c','d'])\n",
    "print(len(root.child_nodes) == 3)\n",
    "print(root.child_nodes[0].feature_name == 'b')\n",
    "print(root.child_nodes[1].feature_name == 'c')\n",
    "print(root.child_nodes[2].feature_name == 'd')\n",
    "root.child_nodes[0].update_scores_up(1)\n",
    "print(root.child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].get_score() == 1)\n",
    "print(root.get_score() == 1)\n",
    "print(root.T == 1)\n",
    "root.child_nodes[1].update_scores_up(0.8)\n",
    "print(root.child_nodes[1].T == 1)\n",
    "print(root.child_nodes[1].get_score() == 0.8)\n",
    "print(root.get_score() == 0.9)\n",
    "print(root.T == 2)\n",
    "root.child_nodes[2].update_scores_up(0.6)\n",
    "print(root.child_nodes[2].T == 1)\n",
    "print(root.child_nodes[2].get_score() == 0.6)\n",
    "print(root.get_score() == 2.4/3)\n",
    "print(root.T == 3)\n",
    "\n",
    "# Another level\n",
    "print(\"Scenario 3\")\n",
    "root.child_nodes[0].add_child_nodes(['c','d'])\n",
    "print(len(root.child_nodes[0].child_nodes) == 2)\n",
    "print(root.child_nodes[0].child_nodes[0].feature_name == 'c')\n",
    "print(root.child_nodes[0].child_nodes[1].feature_name == 'd')\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full == False)\n",
    "root.child_nodes[0].child_nodes[0].update_scores_up(0.2)\n",
    "print(root.child_nodes[0].child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].get_score() == 0.2)\n",
    "print(root.child_nodes[0].T == 2)\n",
    "print(root.child_nodes[0].get_score() == 0.6)\n",
    "print(root.T == 4)\n",
    "print(root.get_score() == 2.6/4)\n",
    "# Scenario 3: last \n",
    "print(\"Scenario 4:\")\n",
    "root.child_nodes[0].child_nodes[0].add_child_nodes(['d'])\n",
    "print(len(root.child_nodes[0].child_nodes[0].child_nodes) == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0]._is_subtree_full)\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full == False)\n",
    "root.child_nodes[0].child_nodes[0].child_nodes[0].update_scores_up(0.4)\n",
    "print(root.child_nodes[0].child_nodes[0]._is_subtree_full)\n",
    "print(root.child_nodes[0].child_nodes[1]._is_subtree_full == False)\n",
    "print(root.child_nodes[0]._is_subtree_full == False)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0].T == 1)\n",
    "print(root.child_nodes[0].child_nodes[0].child_nodes[0].get_score() == 0.4)\n",
    "print(root.child_nodes[0].child_nodes[0].T == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiarm strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiArmStrategies:\n",
    "    def __init__(self, name, all_node_names):\n",
    "        self._name = name\n",
    "        self._all_node_names = all_node_names\n",
    "        \n",
    "    def multiarm_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        if(self._name == 'default'):\n",
    "            return self._default_strategy(node, used_features, scoring_function, other_scores)\n",
    "        else:\n",
    "            return None    \n",
    "    \n",
    "    def _default_strategy(self, node, used_features, scoring_function, other_scores):\n",
    "        #print(\"default strategy: \" + node.feature_name)\n",
    "        if(len(node.child_nodes) == 0):\n",
    "            #print(\"first if\")\n",
    "            self._add_child_nodes(node, used_features)\n",
    "            return node.child_nodes[0]\n",
    "        else:\n",
    "            #print(\"else\")\n",
    "            best_score = 0\n",
    "            best_node = None\n",
    "            tmp_score = 0\n",
    "            \n",
    "            for child_node in node.child_nodes:\n",
    "                if(not child_node._is_subtree_full):\n",
    "                    score = scoring_function(child_node, other_scores)\n",
    "                    if(score > best_score):\n",
    "                        best_score = score\n",
    "                        best_node = child_node\n",
    "            return best_node\n",
    "  \n",
    "    def _add_child_nodes(self, node, used_features):\n",
    "        #print('adding nodes to ' + node.feature_name + ' :' + ' '.join(self._all_node_names - used_features))\n",
    "        node.add_child_nodes(self._all_node_names - used_features)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EndStrategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EndStrategies:\n",
    "    def __init__(self, name):\n",
    "        self._name = name\n",
    "    \n",
    "    def are_calculations_over(self, node, params):\n",
    "        if(self._name == 'default'):\n",
    "            return self._first_new_strategy(node, params)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def _first_new_strategy(self, node, params):\n",
    "        #print(\"_first_new_strategy:\")\n",
    "        if(node.T > 0 and not node._is_subtree_full):\n",
    "            #print(\"first if\")\n",
    "            return False\n",
    "        else:\n",
    "            if(node._parent_node == None):\n",
    "                #print(\"second if, no parent node\")\n",
    "                return False\n",
    "            else:\n",
    "                #print(\"else\")\n",
    "                return True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DefaultSettings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefaultSettings:\n",
    "    @staticmethod\n",
    "    def get_default_params():\n",
    "        return {\n",
    "            \"c_e\": 2,\n",
    "            \"c\": 1,\n",
    "            \"c_l\": 1\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCTS:\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 task = 'classification',\n",
    "                 calculactions_done_conditions = {'type': 'iterations', 'max_val': 10},\n",
    "                 params = None,\n",
    "                 metric = 'acc', \n",
    "                 scoring_function = 'g_rave', \n",
    "                 multiarm_strategy = 'default', \n",
    "                 end_strategy = 'default'):\n",
    "        self._metric_name = metric\n",
    "        self._scoring_function_name = scoring_function\n",
    "        self._multiarm_strategy_name = multiarm_strategy\n",
    "        self._end_strategy_name = end_strategy\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._task = task\n",
    "        self._root = Node(\"\")\n",
    "        self._feature_names = None\n",
    "        self._calculactions_done_conditions = calculactions_done_conditions\n",
    "        self._model = model\n",
    "        self._time = 0\n",
    "        self._iterations = 0\n",
    "        \n",
    "        if(params is None):\n",
    "            self._params = DefaultSettings.get_default_params()\n",
    "        \n",
    "    def fit(self, data, out_variable):\n",
    "        \n",
    "        if(self._task == 'classification'):\n",
    "            self._classification_fit(data, out_variable)\n",
    "        else:\n",
    "            self._regression_fit(data, out_variable)\n",
    "            \n",
    "    def _classification_fit(self, data, out_variable):\n",
    "        self._init_fitting_values(data)\n",
    "        while not self._is_fitting_over():\n",
    "            self._single_classification_iteration(data, out_variable)\n",
    "        \n",
    "        self._model.fit(data.loc[:, self._best_features], out_variable)\n",
    "    \n",
    "    def _regression_fit(self, data, out_variable):\n",
    "        return None\n",
    "    \n",
    "    def _single_classification_iteration(self, data, out_variable):\n",
    "        #print('classification iteration')\n",
    "        used_features = set()\n",
    "        node = self._root\n",
    "        is_iteration_over = False\n",
    "        while not is_iteration_over:\n",
    "            node = self._multiarm_strategy.multiarm_strategy(node, used_features, self._scoring_function.get_score, self._global_scores.scores)\n",
    "            #print(\"selected feature: \" + node.feature_name)\n",
    "            is_iteration_over = self._end_strategy.are_calculations_over(node, self._params)\n",
    "            used_features.add(node.feature_name)\n",
    "            #print(\"--------\")\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(data.loc[:,used_features], out_variable, test_size = 0.25, random_state = 123)\n",
    "        self._model.fit(X_train, y_train)\n",
    "        predicted = self._model.predict(X_test)\n",
    "        score = self._metric(y_test, predicted)\n",
    "        #print('score: ' + str(score))\n",
    "        node.update_scores_up(score, self._global_scores)\n",
    "        #print(used_features)\n",
    "        if(score > self._best_score):\n",
    "            self._best_score = score\n",
    "            self._best_features = used_features\n",
    "        \n",
    "        if(self._longest_tree_branch < len(used_features)):\n",
    "            self._longest_tree_branch = len(used_features)\n",
    "        \n",
    "        #print(\"----------END OF ITERATION----------\")\n",
    "    \n",
    "    def _single_regression_iteration(self):\n",
    "        return None\n",
    "    \n",
    "    def _init_fitting_values(self, data):\n",
    "        self._feature_names = set(data.columns)\n",
    "        self._multiarm_strategy = MultiArmStrategies(self._multiarm_strategy_name, self._feature_names)\n",
    "        self._end_strategy = EndStrategies(self._end_strategy_name)\n",
    "        self._scoring_function = ScoringFunctions(self._scoring_function_name, self._params)\n",
    "        self._metric = BuildInMetrics().get_metric(self._metric_name)\n",
    "        self._best_features = None\n",
    "        self._best_score = 0\n",
    "        self._iterations = 0\n",
    "        self._longest_tree_branch = 0\n",
    "        self._time = time.time()\n",
    "        self._global_scores = GlobalScores()\n",
    "    \n",
    "    def _is_fitting_over(self):\n",
    "        if(self._root._is_subtree_full):\n",
    "            print('Whole tree searched, finishing prematurely')\n",
    "            return True\n",
    "        \n",
    "        if(self._calculactions_done_conditions['type'] == 'iterations'):\n",
    "            self._iterations += 1\n",
    "            return self._iterations > self._calculactions_done_conditions['max_val'] \n",
    "        else:\n",
    "            #print('Time ellapsed: ' + str(time.time() - self._time))\n",
    "            return (time.time() - self._time) > self._calculactions_done_conditions['max_val'] \n",
    "        \n",
    "    def predict(self, data):\n",
    "        return self._model.predict(data.loc[:, self._best_features])\n",
    "    \n",
    "    def predict_proba(self, data):\n",
    "        return self._model.predict_proba(data.loc[:, self._best_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + file_names[6]['name'])\n",
    "data_labels = data.loc[:,file_names[6]['target_class']]\n",
    "data = data.drop(columns = [file_names[6]['target_class']])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, data_labels, test_size = 0.25, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_path + file_names[12]['name'])\n",
    "labels = data['class']\n",
    "labels[labels == -1] = 0\n",
    "data = data.drop(columns = ['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auc: 0.5375857219608258\n",
      "acc: 0.5325\n",
      "f1: 0.5348084947997962\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits = 5, random_state = 123, shuffle=True)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "auc_score = 0\n",
    "acc_score = 0\n",
    "f1_score_ = 0\n",
    "for train, test in kf.split(data):\n",
    "    model.fit(data.loc[train,:], labels[train])\n",
    "    predicted = model.predict(data.loc[test,:])\n",
    "    p_proba = model.predict_proba(data.loc[test,:])[:,1]\n",
    "    auc_score += roc_auc_score(labels[test] == 1, p_proba)\n",
    "    acc_score += accuracy_score(labels[test], predicted)\n",
    "    f1_score_ += f1_score(labels[test], predicted, pos_label = 1)\n",
    "\n",
    "print('auc: ' + str(auc_score/5))\n",
    "print('acc: ' + str(acc_score/5))\n",
    "print('f1: ' + str(f1_score_/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(solver='liblinear')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size = 0.25, random_state = 123)\n",
    "mcts = MCTS(model, calculactions_done_conditions = {'type': 'time', 'max_val': 600}, metric='acc')\n",
    "mcts.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199731045081968\n",
      "0.59\n",
      "0.5841784989858011\n"
     ]
    }
   ],
   "source": [
    "predicted = mcts.predict(X_test)\n",
    "p_proba = mcts.predict_proba(X_test)\n",
    "print(roc_auc_score(y_test == 1, p_proba[:,1]))\n",
    "print(accuracy_score(y_test, predicted))\n",
    "print(f1_score(y_test, predicted, pos_label = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x103',\n",
       " 'x111',\n",
       " 'x113',\n",
       " 'x115',\n",
       " 'x116',\n",
       " 'x121',\n",
       " 'x125',\n",
       " 'x127',\n",
       " 'x130',\n",
       " 'x133',\n",
       " 'x140',\n",
       " 'x141',\n",
       " 'x142',\n",
       " 'x147',\n",
       " 'x156',\n",
       " 'x161',\n",
       " 'x182',\n",
       " 'x186',\n",
       " 'x19',\n",
       " 'x192',\n",
       " 'x197',\n",
       " 'x198',\n",
       " 'x201',\n",
       " 'x206',\n",
       " 'x209',\n",
       " 'x211',\n",
       " 'x216',\n",
       " 'x220',\n",
       " 'x221',\n",
       " 'x225',\n",
       " 'x232',\n",
       " 'x239',\n",
       " 'x244',\n",
       " 'x248',\n",
       " 'x25',\n",
       " 'x250',\n",
       " 'x260',\n",
       " 'x262',\n",
       " 'x265',\n",
       " 'x269',\n",
       " 'x275',\n",
       " 'x281',\n",
       " 'x289',\n",
       " 'x30',\n",
       " 'x300',\n",
       " 'x307',\n",
       " 'x318',\n",
       " 'x32',\n",
       " 'x324',\n",
       " 'x326',\n",
       " 'x33',\n",
       " 'x345',\n",
       " 'x353',\n",
       " 'x358',\n",
       " 'x364',\n",
       " 'x365',\n",
       " 'x367',\n",
       " 'x370',\n",
       " 'x378',\n",
       " 'x379',\n",
       " 'x381',\n",
       " 'x382',\n",
       " 'x391',\n",
       " 'x403',\n",
       " 'x410',\n",
       " 'x412',\n",
       " 'x421',\n",
       " 'x425',\n",
       " 'x427',\n",
       " 'x429',\n",
       " 'x43',\n",
       " 'x432',\n",
       " 'x435',\n",
       " 'x436',\n",
       " 'x440',\n",
       " 'x441',\n",
       " 'x442',\n",
       " 'x445',\n",
       " 'x446',\n",
       " 'x45',\n",
       " 'x453',\n",
       " 'x454',\n",
       " 'x455',\n",
       " 'x46',\n",
       " 'x464',\n",
       " 'x466',\n",
       " 'x469',\n",
       " 'x476',\n",
       " 'x477',\n",
       " 'x480',\n",
       " 'x491',\n",
       " 'x493',\n",
       " 'x494',\n",
       " 'x496',\n",
       " 'x497',\n",
       " 'x498',\n",
       " 'x50',\n",
       " 'x500',\n",
       " 'x56',\n",
       " 'x62',\n",
       " 'x64',\n",
       " 'x76',\n",
       " 'x86',\n",
       " 'x91',\n",
       " 'x94',\n",
       " 'x99'}"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcts._best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
